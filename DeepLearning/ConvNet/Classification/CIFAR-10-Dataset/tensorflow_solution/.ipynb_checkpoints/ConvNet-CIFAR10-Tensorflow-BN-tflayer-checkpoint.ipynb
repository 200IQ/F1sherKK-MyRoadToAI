{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 is a dataset of 60 000 32x32 colour images, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. There are 10 different classes with 6000 images per class:\n",
    "\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "Dataset is divided into 5 batches with the following naming convention 'data_batch_1', 'data_batch_2' and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "DATA_URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "TARG_TZ_FILEPATH = \"./cifar-10-python.tar.gz\"\n",
    "CIFAR10_FILES_FOLDER = './cifar-10-batches-py/'\n",
    "\n",
    "TRAIN_BATCH_FILES = [\n",
    "    \"data_batch_1\",\n",
    "    \"data_batch_2\",\n",
    "    \"data_batch_3\",\n",
    "    \"data_batch_4\",\n",
    "    \"data_batch_5\"\n",
    "]\n",
    "\n",
    "TEST_BATCH_FILE = \"test_batch\"\n",
    "\n",
    "LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "MAX_PIXEL_VALUE = 255\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "DATA_I = 0\n",
    "LABELS_I = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not os.path.isfile(TARG_TZ_FILEPATH):\n",
    "    with DLProgress(unit=\"B\", unit_scale=True, miniters=1, desc=\"CIFAR-10\") as pbar:\n",
    "        urlretrieve(DATA_URL, TARG_TZ_FILEPATH, pbar.hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exctracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(CIFAR10_FILES_FOLDER):\n",
    "    with tarfile.open(TARG_TZ_FILEPATH) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = list()\n",
    "\n",
    "for batch_file in TRAIN_BATCH_FILES: \n",
    "    with open(CIFAR10_FILES_FOLDER + batch_file, mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "        # array of 10000 * 1 length\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # all channels in single 10000 * 3072 length array\n",
    "        batch_data = batch['data']\n",
    "        \n",
    "        # reshaping to 10000 * 3 * 32 * 32\n",
    "        data = batch_data.reshape((len(batch_data), CHANNELS, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        \n",
    "        # fix indexing\n",
    "        data = data.transpose(0, 2, 3, 1)\n",
    "  \n",
    "        train_batches.append((data, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch = None\n",
    "\n",
    "with open(CIFAR10_FILES_FOLDER + TEST_BATCH_FILE, mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "        # array of 10000 * 1 length\n",
    "        labels = batch['labels']\n",
    "        \n",
    "         # all channels in single 10000 * 3072 length array\n",
    "        batch_data = batch['data']\n",
    "        \n",
    "        # reshaping to 10000 * 3 * 32 * 32\n",
    "        data = batch_data.reshape((len(batch_data), CHANNELS, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        \n",
    "        # fix indexing\n",
    "        data = data.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        test_batch = (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present extracted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def present_batch(batch, batch_index):\n",
    "    features, labels = batch \n",
    "    print(\"Batch no.\" + str(batch_index))\n",
    "    print(\"    - Samples in batch: \" + str(len(features)))\n",
    "    print(\"    - Labels num: \" + str(dict(zip(*np.unique(labels, return_counts=True)))))\n",
    "    \n",
    "def present_sample(batch, sample_id):\n",
    "    features, labels = batch \n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print(\"Sample no.\" + str(sample_id))\n",
    "    print(\"    - Min Value: {} Max Value: {}\".format(sample_image.min(), sample_image.max()))\n",
    "    print(\"    - Shape: {}\".format(sample_image.shape))\n",
    "    print(\"    - Label - Label Id: {} Name: {}\".format(sample_label, LABELS[sample_label]))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch no.0\n",
      "    - Samples in batch: 10000\n",
      "    - Labels num: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "Batch no.1\n",
      "    - Samples in batch: 10000\n",
      "    - Labels num: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "Batch no.2\n",
      "    - Samples in batch: 10000\n",
      "    - Labels num: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "Batch no.3\n",
      "    - Samples in batch: 10000\n",
      "    - Labels num: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "Batch no.4\n",
      "    - Samples in batch: 10000\n",
      "    - Labels num: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(train_batches):\n",
    "    present_batch(batch, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample no.7164\n",
      "    - Min Value: 26 Max Value: 246\n",
      "    - Shape: (32, 32, 3)\n",
      "    - Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/5nVlbAiE/4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f20d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "present_sample(train_batches[0], 7164)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train, val , test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data is already shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concat all batches together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list()\n",
    "train_labels = list()\n",
    "for batch in train_batches:\n",
    "    for data, label in zip(batch[DATA_I], batch[LABELS_I]):\n",
    "        train_data.append(data)\n",
    "        train_labels.append(label)\n",
    "assert len(train_data) == len(train_labels)\n",
    "        \n",
    "test_data = list()\n",
    "test_labels = list()\n",
    "for data, label in zip(test_batch[DATA_I], test_batch[LABELS_I]):\n",
    "    test_data.append(data)\n",
    "    test_labels.append(label)\n",
    "assert len(test_data) == len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "split_index = int(len(train_data) * split_ratio)\n",
    "\n",
    "val_data = train_data[:split_index]\n",
    "val_labels = train_labels[:split_index]\n",
    "\n",
    "train_data = train_data[split_index:]\n",
    "train_labels = train_labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 40000\n",
      "Validation samples: 10000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train samples: \" + str(len(train_data)))\n",
    "print(\"Validation samples: \" + str(len(val_data)))\n",
    "print(\"Test samples: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return data / MAX_PIXEL_VALUE\n",
    "\n",
    "def ohe_label(label):\n",
    "    labels_num = len(LABELS)\n",
    "  \n",
    "    encoding_array = np.zeros([labels_num])  \n",
    "    encoding_array[label] = 1\n",
    "        \n",
    "    return encoding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_norm = [normalize(data) for data in train_data]\n",
    "train_labels_ohe = [ohe_label(label) for label in train_labels]\n",
    "\n",
    "val_data_norm = [normalize(data) for data in val_data]\n",
    "val_labels_ohe = [ohe_label(label) for label in val_labels]\n",
    "\n",
    "test_data_norm = [normalize(data) for data in test_data]\n",
    "test_labels_ohe = [ohe_label(label) for label in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample no.7124\n",
      "    - Min Value: 0.00784313725490196 Max Value: 0.9137254901960784\n",
      "    - Label: [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "sample_to_inspect = 7124\n",
    "sample = train_data_norm[sample_to_inspect]\n",
    "label = train_labels_ohe[sample_to_inspect]\n",
    "print(\"Sample no.\" + str(sample_to_inspect))\n",
    "print(\"    - Min Value: {} Max Value: {}\".format(sample.min(), sample.max()))\n",
    "print(\"    - Label: \" + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building ConvNet - parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net_image_input(image_width, image_height, channels):\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_width, image_height, channels], name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net_label_input(n_classes):\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net_keep_prob_input():\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net_is_training_placeholder():\n",
    "    return tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(inputs, keep_prob, is_training):   \n",
    "    \n",
    "    # Conv Layer - 1, Input 32x32x3 -> BatchNorm -> eLU -> MaxPooling -> Output 16x16x32\n",
    "    conv1_layer = tf.layers.conv2d(inputs=inputs, filters=32, kernel_size=3, strides=1, \n",
    "                                   padding=\"SAME\", activation=None,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv1_layer_norm = tf.layers.batch_normalization(conv1_layer, training=is_training)\n",
    "    conv1_layer_activated = tf.nn.elu(conv1_layer_norm)\n",
    "    conv1_layer_mpool = tf.layers.max_pooling2d(conv1_layer_activated, pool_size=2, strides=2)\n",
    "    \n",
    "    # Conv Layer - 2, Input 16x16x16 -> BatchNorm -> eLU -> MaxPooling -> Output 8x8x32\n",
    "    conv2_layer = tf.layers.conv2d(inputs=conv1_layer_mpool, filters=48, kernel_size=3, strides=1, \n",
    "                                   padding=\"SAME\", activation=None, \n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv2_layer_norm = tf.layers.batch_normalization(conv2_layer, training=is_training)\n",
    "    conv2_layer_activated = tf.nn.elu(conv2_layer_norm)\n",
    "    conv2_layer_mpool = tf.layers.max_pooling2d(conv2_layer_activated, pool_size=2, strides=2)\n",
    "\n",
    "    # Conv Layer - 3, Input 8x8x32 -> BatchNorm -> eLU -> MaxPooling -> Output 4x4x64\n",
    "    conv3_layer = tf.layers.conv2d(inputs=conv2_layer_mpool, filters=64, kernel_size=3, strides=1,\n",
    "                                   padding=\"SAME\", activation=None,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    conv3_layer_norm = tf.layers.batch_normalization(conv3_layer, training=is_training)\n",
    "    conv3_layer_activated = tf.nn.elu(conv3_layer_norm)\n",
    "    conv3_layer_mpool = tf.layers.max_pooling2d(conv3_layer_activated, pool_size=2, strides=2)\n",
    "    \n",
    "    # Conv Layer Flattened\n",
    "    conv_output_shape = conv3_layer_mpool.get_shape().as_list()\n",
    "    flatten=tf.reshape(conv3_layer_mpool,\n",
    "                       [-1, conv_output_shape[1] * conv_output_shape[2] * conv_output_shape[3]])\n",
    "    \n",
    "    # Dense Layer - 1, Size 256 -> Dropout\n",
    "    dense1_layer = tf.layers.dense(inputs=flatten, units=256, activation=None,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    dense1_layer_norm = tf.layers.batch_normalization(dense1_layer, training=is_training)\n",
    "    dense1_layer_activated = tf.nn.elu(dense1_layer_norm)\n",
    "    dense1_layer_dropout = tf.layers.dropout(dense1_layer_activated, rate=keep_prob)\n",
    "    \n",
    "    # Dense Layer - 2, Size 128 -> Dropout\n",
    "    dense2_layer = tf.layers.dense(inputs=dense1_layer_dropout, units=128, activation=None,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    dense2_layer_norm = tf.layers.batch_normalization(dense2_layer, training=is_training)\n",
    "    dense2_layer_activated = tf.nn.elu(dense2_layer_norm)\n",
    "    dense2_layer_dropout = tf.layers.dropout(dense2_layer_activated, rate=keep_prob)\n",
    "    \n",
    "    # Output Layer \n",
    "    output = tf.layers.dense(inputs=dense2_layer_dropout, units=NUM_CLASSES, activation=None,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_mini_batches(data, labels, batch_size):\n",
    "    assert len(data) == len(labels)\n",
    "    \n",
    "    all_batches = list()\n",
    "    for i in range(0, len(data)):\n",
    "        all_batches.append((data[i], labels[i]))\n",
    "    random.shuffle(all_batches)\n",
    "        \n",
    "    mini_batches = list()\n",
    "    while len(all_batches) >= batch_size:\n",
    "        \n",
    "        data_batch = list()\n",
    "        labels_batch = list()\n",
    "        for j in range(0, batch_size):\n",
    "            data, labels = all_batches.pop()\n",
    "            data_batch.append(data)\n",
    "            labels_batch.append(labels)\n",
    "            \n",
    "        mini_batches.append((np.array(data_batch), np.array(labels_batch)))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building ConvNet - assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reset Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dropout() got an unexpected keyword argument 'rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1e88fb20e589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_net_is_training_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-a4218492b9e4>\u001b[0m in \u001b[0;36mconv_net\u001b[0;34m(inputs, keep_prob, is_training)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdense1_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdense1_layer_activated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1_layer_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdense1_layer_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1_layer_activated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Dense Layer - 2, Size 128 -> Dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout() got an unexpected keyword argument 'rate'"
     ]
    }
   ],
   "source": [
    "inputs = conv_net_image_input(IMAGE_WIDTH, IMAGE_HEIGHT, CHANNELS)\n",
    "targets = conv_net_label_input(NUM_CLASSES)\n",
    "keep_prob = conv_net_keep_prob_input()\n",
    "is_training = conv_net_is_training_placeholder()\n",
    "\n",
    "model = conv_net(inputs, keep_prob, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost function, training optimizer, correct predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    optimizer = tf.train.AdamOptimizer(0.0005).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "keep_probability = 0.75\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_count = 0\n",
    "info = {'train_cost':[], 'train_acc':[], 'valid_cost':[], 'valid_acc':[]}\n",
    "for epoch in range(epochs):\n",
    "    mini_batches = prepare_mini_batches(train_data_norm, train_labels_ohe, batch_size)\n",
    "\n",
    "    while mini_batches:\n",
    "        data_batch, label_batch = mini_batches.pop()\n",
    "        batch_count += 1\n",
    "\n",
    "        session.run(optimizer, feed_dict={inputs: data_batch, \n",
    "                                          targets: label_batch,\n",
    "                                          keep_prob: keep_probability,\n",
    "                                          is_training: True})\n",
    "        if (batch_count % 50) == 0:\n",
    "            train_cost = session.run(cost, feed_dict={\n",
    "                    inputs: data_batch, targets: label_batch, keep_prob: 1. ,is_training: False})\n",
    "\n",
    "            train_acc = session.run(accuracy, feed_dict={\n",
    "                inputs: data_batch, targets: label_batch, keep_prob: 1., is_training: False})\n",
    "\n",
    "            info[\"train_cost\"].append(train_cost)\n",
    "            info[\"train_acc\"].append(train_acc)\n",
    "\n",
    "            valid_cost = session.run(cost, feed_dict={\n",
    "                inputs: val_data_norm, targets: val_labels_ohe, keep_prob: 1., is_training: False})\n",
    "\n",
    "            valid_acc = session.run(accuracy, feed_dict={\n",
    "                inputs: val_data_norm, targets: val_labels_ohe, keep_prob: 1., is_training: False})\n",
    "\n",
    "            info[\"valid_cost\"].append(valid_cost)\n",
    "            info[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "            print('Epoch {}, Batch {}:  '.format(epoch + 1, batch_count), end='')\n",
    "            print(\"Train cost = \", \"{:.4f},\".format(train_cost), \n",
    "                  \"Train acc = \", \"{:.4f}\".format(train_acc),\n",
    "                  \"Val cost = \", \"{:.4f},\".format(valid_cost), \n",
    "                  \"Val acc = \", \"{:.4f}\".format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(info['train_acc'], label='Training acc')\n",
    "ax1.plot(info['valid_acc'], label='Validation acc')\n",
    "blue_patch = mpatches.Patch(color='#699cef', label='Training accuracy')\n",
    "orange_patch = mpatches.Patch(color='orange', label='Validation accuracy')\n",
    "ax1.legend(handles=[blue_patch, orange_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax2 = fig1.add_subplot(111)\n",
    "ax2.plot(info['train_cost'], label='Training loss')\n",
    "ax2.plot(info['valid_cost'], label='Validation loss')\n",
    "blue_patch = mpatches.Patch(color='#699cef', label='Training cross-entropy')\n",
    "orange_patch = mpatches.Patch(color='orange', label='Validation cross-entropy')\n",
    "ax2.legend(handles=[blue_patch, orange_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = session.run(accuracy, feed_dict={\n",
    "    inputs: test_data_norm, targets: test_labels_ohe, keep_prob: 1., is_training: False})\n",
    "print(\"Test data accuracy: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
